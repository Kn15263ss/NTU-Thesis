{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Perhaps you called ray.init twice by accident? This error can be suppressed by passing in 'ignore_reinit_error=True' or by calling 'ray.shutdown()' prior to 'ray.init()'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2324e5b12db2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     sched = AsyncHyperBandScheduler(\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/ray/worker.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(redis_address, num_cpus, num_gpus, resources, object_store_memory, redis_max_memory, node_ip_address, object_id_seed, num_workers, local_mode, driver_mode, redirect_worker_output, redirect_output, ignore_reinit_error, num_redis_shards, redis_max_clients, redis_password, plasma_directory, huge_pages, include_webui, driver_id, configure_logging, logging_level, logging_format, plasma_store_socket_name, raylet_socket_name, temp_dir, _internal_config, use_raylet)\u001b[0m\n\u001b[1;32m   1362\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1364\u001b[0;31m             raise Exception(\"Perhaps you called ray.init twice by accident? \"\n\u001b[0m\u001b[1;32m   1365\u001b[0m                             \u001b[0;34m\"This error can be suppressed by passing in \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m                             \u001b[0;34m\"'ignore_reinit_error=True' or by calling \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Perhaps you called ray.init twice by accident? This error can be suppressed by passing in 'ignore_reinit_error=True' or by calling 'ray.shutdown()' prior to 'ray.init()'."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import argparse\n",
    "import utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import Trainable\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest import BayesOptSearch\n",
    "from model import Vgg11, Resnet18, MobileNet, MobileNetV2\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "torch.cuda.manual_seed_all(50)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--Dataset_name', type=str, default='')\n",
    "parser.add_argument('--Network_name', type=str, default='')\n",
    "\n",
    "args, unparsed = parser.parse_known_args()\n",
    "\n",
    "class HyperTrain(Trainable):\n",
    "\n",
    "    def _get_dataset(self, name):\n",
    "\n",
    "        normalize = transforms.Normalize(\n",
    "            mean=[0.4914, 0.4822, 0.4465],\n",
    "            std=[0.2023, 0.1994, 0.2010],\n",
    "        )\n",
    "\n",
    "        if name == 'FashionMNIST':\n",
    "\n",
    "            data_transforms = transforms.Compose([\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.ToTensor(),\n",
    "                normalize])\n",
    "            dataset = torchvision.datasets.FashionMNIST(root=\"/home/willy-huang/workspace/data/FashionMNIST\",\n",
    "                                                        transform=data_transforms)\n",
    "            num_classes = 10\n",
    "            input_size = 512 * 1 * 1\n",
    "\n",
    "            return dataset, num_classes, input_size\n",
    "\n",
    "        elif name == 'CIFAR10':\n",
    "\n",
    "            data_transforms = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                normalize])\n",
    "            dataset = torchvision.datasets.CIFAR10(root=\"/home/willy-huang/workspace/data/CIFAR10/\",\n",
    "                                                   transform=data_transforms)\n",
    "            num_classes = 10\n",
    "            input_size = 512 * 1 * 1\n",
    "\n",
    "            return dataset, num_classes, input_size\n",
    "\n",
    "        elif name == 'SVHN':\n",
    "\n",
    "            data_transforms = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                normalize])\n",
    "            dataset = torchvision.datasets.SVHN(root=\"/home/willy-huang/workspace/data/SVHN/\",\n",
    "                                                transform=data_transforms)\n",
    "            num_classes = 10\n",
    "            input_size = 512 * 1 * 1\n",
    "\n",
    "            return dataset, num_classes, input_size\n",
    "\n",
    "        elif name == 'STL10':\n",
    "\n",
    "            data_transforms = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                normalize])\n",
    "            dataset = torchvision.datasets.STL10(root=\"/home/willy-huang/workspace/data/STL10/\",\n",
    "                                                 transform=data_transforms)\n",
    "            num_classes = 10\n",
    "            input_size = 512 * 3 * 3\n",
    "\n",
    "            return dataset, num_classes, input_size\n",
    "\n",
    "        # elif name == 'Food':\n",
    "        #\n",
    "        #     class Food(Dataset):\n",
    "        #\n",
    "        #         def __init__(self, files, class_names, transform=transforms.ToTensor()):\n",
    "        #\n",
    "        #             self.data = files\n",
    "        #             self.transform = transform\n",
    "        #             self.class_names = class_names\n",
    "        #\n",
    "        #         def __getitem__(self, idx):\n",
    "        #             img = Image.open(self.data[idx]).convert('RGB')\n",
    "        #             name = self.data[idx].split('/')[-2]\n",
    "        #             y = self.class_names.index(name)\n",
    "        #             img = self.transform(img)\n",
    "        #             return img, y\n",
    "        #\n",
    "        #         def __len__(self):\n",
    "        #             return len(self.data)\n",
    "        #\n",
    "        #     data_transforms = transforms.Compose([\n",
    "        #         transforms.RandomHorizontalFlip(),\n",
    "        #         transforms.RandomVerticalFlip(),\n",
    "        #         transforms.Resize((224, 224)),\n",
    "        #         transforms.ToTensor(),\n",
    "        #         normalize])\n",
    "        #\n",
    "        #     path = '/home/willy-huang/workspace/data/food'\n",
    "        #     files_training = glob(os.path.join(path, '*/*.jpg'))\n",
    "        #     class_names = []\n",
    "        #\n",
    "        #     for folder in os.listdir(os.path.join(path)):\n",
    "        #         class_names.append(folder)\n",
    "        #\n",
    "        #     num_classes = len(class_names)\n",
    "        #     dataset = Food(files_training, class_names, data_transforms)\n",
    "        #     input_size = 512 * 7 * 7\n",
    "        #\n",
    "        #     return dataset, num_classes, input_size\n",
    "        #\n",
    "        # elif name == 'Stanford_dogs':\n",
    "        #\n",
    "        #     class Stanford_dogs(Dataset):\n",
    "        #\n",
    "        #         def __init__(self, files, class_names, transform=transforms.ToTensor()):\n",
    "        #\n",
    "        #             self.data = files\n",
    "        #             self.transform = transform\n",
    "        #             self.class_names = class_names\n",
    "        #\n",
    "        #         def __getitem__(self, idx):\n",
    "        #             img = Image.open(self.data[idx]).convert('RGB')\n",
    "        #             name = self.data[idx].split('/')[-2]\n",
    "        #             y = self.class_names.index(name)\n",
    "        #             img = self.transform(img)\n",
    "        #             return img, y\n",
    "        #\n",
    "        #         def __len__(self):\n",
    "        #             return len(self.data)\n",
    "        #\n",
    "        #\n",
    "        #     data_transforms = transforms.Compose([\n",
    "        #         transforms.RandomHorizontalFlip(),\n",
    "        #         transforms.RandomVerticalFlip(),\n",
    "        #         transforms.Resize((224, 224)),\n",
    "        #         transforms.ToTensor(),\n",
    "        #         normalize])\n",
    "        #\n",
    "        #     path = '/home/willy-huang/workspace/data/stanford_dogs'\n",
    "        #     files_training = glob(os.path.join(path, '*/*.jpg'))\n",
    "        #     class_names = []\n",
    "        #\n",
    "        #     for folder in os.listdir(os.path.join(path)):\n",
    "        #         class_names.append(folder)\n",
    "        #\n",
    "        #     num_classes = len(class_names)\n",
    "        #     dataset = Stanford_dogs(files_training, class_names, data_transforms)\n",
    "        #     input_size = 512 * 7 * 7\n",
    "        #\n",
    "        #     return dataset, num_classes, input_size\n",
    "\n",
    "    def _setup(self, config):\n",
    "        self.start_time = time.time()\n",
    "        self.name = args.Dataset_name\n",
    "        nnArchitecture = args.Network_name\n",
    "\n",
    "        dataset, num_class, input_size = self._get_dataset(self.name)\n",
    "\n",
    "        num_total = len(dataset)\n",
    "        shuffle = np.random.permutation(num_total)\n",
    "        split_val = int(num_total * 0.2)\n",
    "\n",
    "        train_idx, valid_idx = shuffle[split_val:], shuffle[:split_val]\n",
    "\n",
    "        train_sampler = SubsetRandomSampler(train_idx)\n",
    "        valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "        self.trainset_ld = DataLoader(dataset, batch_size=128, sampler=train_sampler, num_workers=4)\n",
    "        self.validset_ld = DataLoader(dataset, batch_size=128, sampler=valid_sampler, num_workers=4)\n",
    "\n",
    "        self.modelname = '{}--{}.pth.tar'.format(self.name, nnArchitecture)\n",
    "        loggername = self.modelname.replace(\"pth.tar\", \"log\")\n",
    "        self.logger = utils.buildLogger(loggername)\n",
    "\n",
    "        self.seed_table = np.array([\"\",\"epoch\",\"lr\",\"momentum\",\"weight_decay\",\"factor\",\"outLoss\",\"accuracy\"])\n",
    "\n",
    "        # ---- hyperparameters ----\n",
    "        self.lr = config[\"lr\"]\n",
    "        self.momentum = config[\"momentum\"]\n",
    "        self.weight_decay = config[\"weight_decay\"]\n",
    "        self.factor = config[\"factor\"]\n",
    "\n",
    "        self.epochID = 0\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.accuracy = -999999999999.0\n",
    "\n",
    "        # -------------------- SETTINGS: NETWORK ARCHITECTURE\n",
    "\n",
    "        if nnArchitecture == 'Vgg11':\n",
    "            self.model = Vgg11(num_class, input_size).cuda()\n",
    "\n",
    "        elif nnArchitecture == 'Resnet18':\n",
    "            self.model = Resnet18(num_class, input_size).cuda()\n",
    "\n",
    "        elif nnArchitecture == 'MobileNet':\n",
    "            self.model = MobileNet(num_class, input_size).cuda()\n",
    "\n",
    "        elif nnArchitecture == 'MobileNetV2':\n",
    "            self.model = MobileNetV2(num_class, input_size).cuda()\n",
    "\n",
    "        else:\n",
    "            self.model = None\n",
    "            assert 0\n",
    "\n",
    "        self.model = torch.nn.DataParallel(self.model).cuda()\n",
    "        self.logger.info(\"Build Model Done\")\n",
    "\n",
    "        # -------------------- SETTINGS: OPTIMIZER & SCHEDULER --------------------\n",
    "        self.optimizer = optim.SGD(filter(lambda x: x.requires_grad, self.model.parameters()),\n",
    "                                   lr=self.lr,\n",
    "                                   momentum=self.momentum,\n",
    "                                   weight_decay=self.weight_decay,\n",
    "                                   nesterov=False)\n",
    "\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer,\n",
    "                                                              factor=self.factor,\n",
    "                                                              patience=10, mode='min')\n",
    "\n",
    "        self.logger.info(\"Build Optimizer Done\")\n",
    "\n",
    "    def _train_iteration(self):\n",
    "\n",
    "        self.model.train()\n",
    "\n",
    "        for batchID, (input, target) in enumerate(self.trainset_ld):\n",
    "            varInput = Variable(input).cuda(async=True)\n",
    "            varTarget = Variable(target).cuda(async=True)\n",
    "            varOutput = self.model(varInput)\n",
    "\n",
    "            lossvalue = self.loss(varOutput, varTarget)\n",
    "            self.optimizer.zero_grad()\n",
    "            lossvalue.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def _test(self):\n",
    "\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        lossVal = 0\n",
    "        lossValNorm = 0\n",
    "        correct = 0\n",
    "\n",
    "        num_samples = 0\n",
    "        for batchID, (input, target) in enumerate(self.validset_ld):\n",
    "            with torch.no_grad():\n",
    "                varInput = Variable(input).cuda(async=True)\n",
    "                varTarget = Variable(target).cuda(async=True)\n",
    "                varOutput = self.model(varInput)\n",
    "\n",
    "                losstensor = self.loss(varOutput, varTarget)\n",
    "\n",
    "                pred = varOutput.argmax(1)\n",
    "                correct += (pred == varTarget).sum().cpu()\n",
    "\n",
    "                lossVal += losstensor.item()\n",
    "                lossValNorm += 1\n",
    "                num_samples += len(input)\n",
    "\n",
    "        self.outLoss = lossVal / lossValNorm\n",
    "        accuracy = correct.item() / num_samples\n",
    "\n",
    "        self.scheduler.step(self.outLoss, epoch=self.epochID)\n",
    "\n",
    "        if accuracy > self.accuracy:\n",
    "            self.accuracy = accuracy\n",
    "\n",
    "            torch.save({'epoch': self.epochID + 1,\n",
    "                        'state_dict': self.model.state_dict(),\n",
    "                        'loss': self.outLoss,\n",
    "                        'best_accuracy': self.accuracy,\n",
    "                        'optimizer': self.optimizer.state_dict(),\n",
    "                        }, \"./best_\"+self.modelname)\n",
    "\n",
    "            save = np.array([self.seed_table,\n",
    "                             [str(self.name), str(self.epochID+1), str(self.lr),\n",
    "                              str(self.momentum), str(self.weight_decay), str(self.factor),\n",
    "                              str(self.outLoss), str(self.accuracy)]])\n",
    "\n",
    "            np.savetxt(\"./seed(50).csv\", save, delimiter=',', fmt=\"%s\")\n",
    "\n",
    "        self.logger.info('Epoch [' + str(self.epochID + 1) + '] loss= {:.5f}'.format(self.outLoss) +\n",
    "                         ' ---- accuracy= {:.5f}'.format(accuracy) +\n",
    "                         ' ---- best_accuracy= {:.5f}'.format(self.accuracy) +\n",
    "                         ' ---- model: {}'.format(self.modelname) +\n",
    "                         ' ---- time: {:.1f} s'.format((time.time() - self.start_time)))\n",
    "\n",
    "        self.epochID += 1\n",
    "\n",
    "        return {\"mean_loss\": self.outLoss, \"mean_accuracy\": accuracy, \"epoch\": self.epochID}\n",
    "\n",
    "    def _train(self):\n",
    "        self._train_iteration()\n",
    "        return self._test()\n",
    "\n",
    "    def _save(self, checkpoint_dir):\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, \"final_model.pth\")\n",
    "        torch.save({\n",
    "            \"epoch\": self.epochID,\n",
    "            \"best_accuracy\": self.accuracy,\n",
    "            'loss': self.outLoss,\n",
    "            \"state_dict\": self.model.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "        }, checkpoint_path)\n",
    "        return checkpoint_path\n",
    "\n",
    "    def _restore(self, checkpoint_path):\n",
    "        self.model.load_state_dict(checkpoint_path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    ray.init()\n",
    "\n",
    "    sched = AsyncHyperBandScheduler(\n",
    "        time_attr=\"training_iteration\",\n",
    "        reward_attr=\"neg_mean_loss\",\n",
    "        max_t=100,\n",
    "        grace_period=5)\n",
    "\n",
    "    space = {\n",
    "        \"lr\": (0.001, 0.1),\n",
    "        \"momentum\": (0.10001, 0.900001)\n",
    "    }\n",
    "\n",
    "\n",
    "    algo = BayesOptSearch(\n",
    "        space,\n",
    "        max_concurrent=4,\n",
    "        reward_attr=\"neg_mean_loss\",\n",
    "        utility_kwargs={\n",
    "            \"kind\": \"ucb\",\n",
    "            \"kappa\": 2.5,\n",
    "            \"xi\": 0.0\n",
    "        }\n",
    "    )\n",
    "    exp_name = \"{}.{}_result\".format(args.Dataset_name, args.Network_name)\n",
    "    tune.run_experiments(\n",
    "        {\n",
    "            exp_name: {\n",
    "                \"stop\": {\n",
    "                    # \"mean_accuracy\": 0.98,\n",
    "                    # \"training_iteration\": 100\n",
    "                    \"epoch\": 50\n",
    "                },\n",
    "                \"resources_per_trial\": {\n",
    "                    \"cpu\": 8,\n",
    "                    \"gpu\": 1\n",
    "                },\n",
    "                \"run\": HyperTrain,\n",
    "                \"checkpoint_at_end\": True,\n",
    "                # \"num_samples\": 20,\n",
    "                \"config\": {\n",
    "                    \"lr\": tune.grid_search([0.01, 0.1]),\n",
    "                    \"momentum\": tune.grid_search([0.1, 0.9]),\n",
    "                    \"weight_decay\": tune.grid_search([1e-4, 1e-6]),\n",
    "                    \"factor\": tune.grid_search([0.1, 0.5])\n",
    "                    # \"lr\": tune.sample_from(\n",
    "                    #     lambda spec: np.random.uniform(0.001, 0.1)),\n",
    "                    # \"momentum\": tune.sample_from(\n",
    "                    #     lambda spec: np.random.uniform(0.1, 0.9))\n",
    "                },\n",
    "                \"local_dir\" : \"/home/willy-huang/workspace/research/ray_results\",\n",
    "            }\n",
    "        },\n",
    "        verbose=1,\n",
    "        # search_alg=algo,\n",
    "        # scheduler=sched\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
